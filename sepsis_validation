library(randomForest)
library(caret)   # for confusionMatrix (if you want more detailed metrics)
library(logger)  # for logging
library(pROC)
library(ggplot2)


# ======================
#  Logging
# ======================
log_dir  <- "log directory"
if (!dir.exists(log_dir)) dir.create(log_dir, recursive = TRUE)
log_file <- file.path(log_dir, "validation_log_file.log")
log_appender(appender_file(log_file))
log_threshold(INFO)

log_info("=== SCRIPT STARTED at {format(Sys.time(), '%Y-%m-%d %H:%M:%S')} ===")

# ======================
# 1) Load Training Data
# ======================
train_data_path <- "train_data.csv"
mice_imputed    <- read.csv(train_data_path, check.names = TRUE)
log_info("Loaded training data from: {train_data_path}")
log_info("Training data dimensions: {nrow(mice_imputed)} rows × {ncol(mice_imputed)} columns")


if ("sepsis" %in% colnames(mice_imputed)) {
  tab_train <- table(mice_imputed$sepsis)
  log_info("Training class distribution (sepsis): {paste(names(tab_train), tab_train, sep='=', collapse=', ')}")
} else {
  log_warn("Training data has no 'sepsis' column at load time")
}

# ======================
# Load Validation  Data
# ======================
test_data_path <- "Gtest_data.csv"
test_data_raw  <- read.csv(test_data_path, check.names = TRUE)
log_info("Loaded validation data from: {test_data_path}")
log_info("Validation data dimensions: {nrow(test_data_raw)} rows × {ncol(test_data_raw)} columns")

# Log class distribution in validation set
if ("sepsis" %in% colnames(test_data_raw)) {
  tab_val <- table(test_data_raw$sepsis)
  log_info("Validation class distribution (sepsis): {paste(names(tab_val), tab_val, sep='=', collapse=', ')}")
} else {
  log_warn("Validation data has no 'sepsis' column at load time")
}

# ======================
#  Specify Features and Target
# ======================
selected_feature_names <- c(
  "CCL3_mean", "CXCL10_mean", "IL.10_mean", "IL.18_mean", "IL.6_mean", "IL.8_mean",
  "PD.L1_mean", "TNFR1_mean", "age", "albumin", "alc_d1", "apache", "bilirubin",
  "bun", "cr", "dbp_min", "gcs", "hct", "lactate", "lps_18h_il6", "lps_18h_tnf",
  "sofa_d1", "sofa_max", "temp"
  , "shock", "vent"
)
target_variable <- "sepsis"

log_info("Selected features ({length(selected_feature_names)}): {paste(selected_feature_names, collapse=', ')}")
log_info("Target variable: {target_variable}")


mice_imputed[[target_variable]] <- factor(mice_imputed[[target_variable]], levels = c(0, 1))
if (!target_variable %in% colnames(test_data_raw)) {
  stop("Target variable ('", target_variable, "') not found in validation dataset.")
}
test_data_raw[[target_variable]] <- factor(test_data_raw[[target_variable]], levels = c(0, 1))


missing_in_train <- setdiff(selected_feature_names, colnames(mice_imputed))
if (length(missing_in_train) > 0) {
  stop("Missing in training data: ", paste(missing_in_train, collapse = ", "))
}
missing_in_test <- setdiff(selected_feature_names, colnames(test_data_raw))
if (length(missing_in_test) > 0) {
  stop("Missing in validation data: ", paste(missing_in_test, collapse = ", "))
}
log_info("All selected features present in both training and validation sets.")


train_summary <- data.frame(
  Feature = selected_feature_names,
  Mean    = sapply(selected_feature_names, function(col) mean(mice_imputed[[col]], na.rm = TRUE)),
  SD      = sapply(selected_feature_names, function(col) sd(mice_imputed[[col]], na.rm = TRUE))
)
log_info("Training feature summaries (first 5):")
for (j in seq_len(min(5, nrow(train_summary)))) {
  log_info("  {train_summary$Feature[j]}: mean={round(train_summary$Mean[j],4)}, sd={round(train_summary$SD[j],4)}")
}
if (nrow(train_summary) > 5) {
  log_info("  ... {nrow(train_summary) - 5} more features logged")
}


val_summary <- data.frame(
  Feature = selected_feature_names,
  Mean    = sapply(selected_feature_names, function(col) mean(test_data_raw[[col]], na.rm = TRUE)),
  SD      = sapply(selected_feature_names, function(col) sd(test_data_raw[[col]], na.rm = TRUE))
)
log_info("Validation feature summaries (first 5):")
for (j in seq_len(min(5, nrow(val_summary)))) {
  log_info("  {val_summary$Feature[j]}: mean={round(val_summary$Mean[j],4)}, sd={round(val_summary$SD[j],4)}")
}
if (nrow(val_summary) > 5) {
  log_info("  ... {nrow(val_summary) - 5} more features logged")
}


X_test <- test_data_raw[, selected_feature_names, drop = FALSE]
y_test <- test_data_raw[[target_variable]]

# -------------------------------
# 10 RF runs 
# -------------------------------
n_runs <- 10
all_rf_metrics <- vector("list", n_runs)
all_importances <- vector("list", n_runs)

for (i in seq_len(n_runs)) {

  this_seed <- 123 + i
  set.seed(this_seed)
  log_info("Run {i} | seed = {this_seed}")

 
  train_prev <- mean(mice_imputed[[target_variable]] == 1)
  class_wt   <- c("0" = 1 / (1 - train_prev),
                  "1" = 1 / train_prev)

 
  rf_model_i <- randomForest(
    x          = mice_imputed[, selected_feature_names],
    y          = mice_imputed[[target_variable]],
    ntree      = 1000,
    mtry       = floor(sqrt(length(selected_feature_names))),
    importance = TRUE,
    classwt    = class_wt
  )
  log_info("Run {i} | RF trained (ntree={rf_model_i$ntree}, mtry={rf_model_i$mtry})")


  oob_prob   <- rf_model_i$votes[, "1"]              # OOB class-1 prob
  oob_truth  <- mice_imputed[[target_variable]]
  roc_train  <- pROC::roc(oob_truth, oob_prob, quiet = TRUE)
  opt_cut    <- pROC::coords(roc_train, "best", best.method = "youden",
                             transpose = FALSE)$threshold
  log_info("Run {i} | Youden-J cut-off (OOB) = {round(opt_cut, 3)}")

  ## 4·4  ONE-SHOT prediction on validation (“exam”) set
  rf_prob_val <- predict(rf_model_i, X_test, type = "prob")[, "1"]
  rf_pred_val <- ifelse(rf_prob_val >= opt_cut, "1", "0")

 
  cm          <- table(True = y_test, Pred = rf_pred_val)
  acc         <- sum(diag(cm)) / sum(cm)
  sens        <- if (sum(cm["1", ]) == 0) NA else cm["1", "1"] / sum(cm["1", ])
  spec        <- if (sum(cm["0", ]) == 0) NA else cm["0", "0"] / sum(cm["0", ])
  prec        <- if (sum(cm[, "1"]) == 0) NA else cm["1", "1"] / sum(cm[, "1"])
  f1          <- if (!is.na(prec) && !is.na(sens) && prec + sens > 0)
                   2 * prec * sens / (prec + sens) else NA
  auc_val     <- as.numeric(pROC::auc(y_test, rf_prob_val))

  all_rf_metrics[[i]] <- data.frame(
    Run = i, Accuracy = acc, Sensitivity = sens,
    Specificity = spec, Precision = prec, F1 = f1, AUC = auc_val
  )

  log_info(
    "Run {i} | Val: Acc={round(acc,3)}  Sens={round(sens,3)}  ",
    "Spec={round(spec,3)}  F1={round(f1,3)}  AUC={round(auc_val,3)}"
  )


  write.csv(
    data.frame(
      PatientID   = if ("patient_id" %in% names(test_data_raw))
                      test_data_raw$patient_id
                    else seq_len(nrow(test_data_raw)),
      Actual      = y_test,
      Predicted   = rf_pred_val,
      Prob_Sepsis = rf_prob_val
    ),
    file = sprintf("predictions_run%02d.csv", i),
    row.names = FALSE
  )


  all_importances[[i]] <- data.frame(
    Feature          = rownames(rf_model_i$importance),
    MeanDecreaseGini = rf_model_i$importance[, "MeanDecreaseGini"],
    stringsAsFactors = FALSE
  )
}



rf_summary <- do.call(rbind, all_rf_metrics)


rf_mean <- as.data.frame(t(colMeans(rf_summary, na.rm = TRUE)))
rf_sd   <- as.data.frame(t(apply(rf_summary, 2, sd, na.rm = TRUE)))


rf_stats <- rbind(mean = rf_mean, sd = rf_sd)
rf_stats <- cbind(stat = rownames(rf_stats), rf_stats)
rownames(rf_stats) <- NULL


write.csv(rf_summary,
          file = file.path(log_dir, "rf_validation_summary.csv"),
          row.names = FALSE)


write.csv(rf_stats,
          file = file.path(log_dir, "rf_validation_summary_stats.csv"),
          row.names = FALSE)

log_info("=== SCRIPT FINISHED at {format(Sys.time(), '%Y-%m-%d %H:%M:%S')} ===")
